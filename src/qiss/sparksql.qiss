/
/  load ten row baby csv table into a dataframe with headers
/
df:c sparksqlreadcsv "iowa_liquor_sales_fixed_with_header10.csv"   / csv -> df
/
/ dataframe operations
/
show df                       / sparksqlshow
first df                      / pickoff the first row as a dataframe
count df                      / -> 10
count distinct df             / drop duplicates  -> 9
3 take df                     / take the first three rows.  returns a 3 row dataframe.
count df * 3 take df          / sparksqljoin (cartesian)  10 rows * 3 rows -> 30 rows
count df,df                   / -> 20 join(append) op is a unionall op for spark sql dataframes
df union df                   / explicit sparksqlunionall
df inter df                   / -> 10 sparksqlinter
df except df                  / -> 0  sparksqlexcept
df ~ df                       / 1b (true) unlike pandas, spark sql has no equals op. we use sparksqlexcept
/
/ single char operations still apply
/
*df                           / sparksqllimit(1)
#df                           / sparksqlcount
?df                           / sparksqldropduplicates
3#df                          / sparksqllimit (note that neg values returns an empty DF)
df * 3#df                     / sparksqljoin (cartesian when no column args are missing)
df,df                         / sparksqlunionall
rows:3 sparksqltake df        / returns type Rows (not DataFrame)
sparksqlprintschema df        / meta
sparksqlexplain[df;1b]        / super meta
sparksqlshow[2;df;0b]         / show first two rows with no truncation
sparksqlrowtovec'rows         / each of the three rows to vec
sparksqlcolumns df            / equiv to cols
sparksqlselect[df;"DATE";"NAME";"ADDRESS"; "STORE"]           / cherry pick cols
sparksqlselectexpr[df;"NAME as newname";"abs(ZIPCODE * -2)"]  / expressions on cols
/
/ table operations (not dataframe)
/
sparksqlregisterdataframeastable[c;df;"myfirsttable"]       / df -> table
tables c                      / sparksqltablenames c        / is it really there? ---> (myfirsttable)
c sparksql "SELECT NAME, DATE, ZIPCODE from myfirsttable"   / we can apply a SQL query func
df:sparksqltable[c; "myfirsttable"]                         / table -> df
                                                            / now go back the other way..
/
/
/
/ df:sparksqlload[ c;"iowa-liquor.csv";"com.databricks.spark.csv"]
/
/ $ wc iowa_liquor_sales_fixed_with_header5.csv
       10     224    2621 iowa_liquor_sales_fixed_with_header5.csv
/ $
/ ~800MB file
/ http://mishadoff.com/blog/spark-in-clojure/
/
/
/
/ Don't put a newline at EOF: the grammar needs work
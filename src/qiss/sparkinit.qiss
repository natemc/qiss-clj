/ spark init
/   spark stand-alone cluster setup:
/     1. git clone https://github.com/apache/spark.git
/        cd spark && git checkout tags/v1.5.1 && build/mvn -DskipTests clean package
/     2. export SPARK_HOME=`pwd`      # above git directory as SPARK_HOME
/     3. mkdir $SPARK_HOME/workerdir
/     4. cp $SPARK_HOME/conf/spark-env-sh.template $SPARK_HOME/conf/spark-env.sh
/     5. edit $SPARK_HOME/conf/spark-env.sh to include the following:
/          export SPARK_WORKER_MEMORY=1g
/          export SPARK_EXECUTOR_MEMORY=512m
/          export SPARK_EXECUTOR_INSTANCES=4
/          export SPARK_WORKER_CORES=2
/          export SPARK_WORKER_DIR=$SPARK_HOME/workerdir
/          export SPARK_WORKER_INSTANCES=4
/     6. cp $SPARK_HOME/conf/slaves.template $SPARK_HOME/conf/slaves   # slave hostname goes here (localhost is default)
/     7. $SPARK_HOME/sbin/start-all.sh   # be sure you have ssh enabled (http://bluishcoder.co.nz/articles/mac-ssh.html)
/     8. tail -f $SPARK_HOME/logs/*      # to observe master/slave log output
/
/ MASTER:"local[*]"
/ c:sparksqlcontext sc:sparkcontext "local[*]" sparkconf  "qiss-local-cluster"
/
/ MASTER:"spark://macbookpro15.local:7077"
sc:sparkcontext "spark://Nathans-iMac.home:7077" sparkconf "qiss-stand-alone-cluster";
c:sparksqlcontext sc;
ssc:sc sparkstreamingcontext 2000;  / spark streaming context with 2 second batch interval
/ streaming test code
/ stream0:sparkstreamingkafkastream[ssc;"localhost:2181";"word-count"; "test";1 ];
/ stream1:sparkstreamingmap[stream0; {key x}];
/ stream2:sparkstreamingflatmap[stream1; {key x}];
/ stream3:sparkstreamingmapw[stream2; {}];
/ stream3:stream2;
/ stream4:sparkstreamingreducebykeyandwindow[ stream3 ; {}; 10 * 60 * 1000; 2000];
/ stream4:sparkstreamingcountbywindow[ stream3 ; 10 * 60 * 1000; 2000];
/ stream4:sparkstreamingmaptopair[stream3; {}];
/ stream5:sparkstreaminggroupbykeyandwindow[ stream4 ; 10 * 60 * 1000; 2000];
/ sparkstreamingprint stream5;
/ futureproducelines[]
/ sparkstreamingstart ssc;
/ sparkstreamingawaittermination ssc
/                  sc --> spark context handle
/                   c --> spark sql context handle
/                 ssc --> spark streaming context handle
/ spark-sql init
/ c:sparksqlcontext sc  TODO: sc is not available for when this line executes during start-up
/ tname:([]a:9#1 2 3;b:9#10 20 30;cee:9#10 20 30)
/ .z.exit:{sparkstop sc}  /  as of spark 1.5, the spark shutdown is triggered automatically
/ Don't put a newline at EOF: the grammar needs work
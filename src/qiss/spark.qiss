

/ https://gorillalabs.github.io/sparkling/articles/getting_started.html

/ c:sparkconf["local";"qiss-sparkling-example"]   / initialize and create a spark config

/ spark cluster can run distributed or locally.  argument below 'local[4]' kicks off a local cluster with 4 threads
/ local[*] --> Run Spark locally with as many worker threads as logical cores on your machine.
sc:sparkcontext[c:sparkconf["local[*]";"qiss-sparkling-example"]]
rdd1:sparkparallelize[sc; (1; 2; 3; 4; 5)]
rdd2:sparkparallelize[sc; (2; 2; 2; 222; 22)]
sparkcollect rdd1 sparkintersection rdd2
sparkcollect sparkparallelize[sc; !10] sparksubtract sparkparallelize[sc; !5]
sparkcollect sparksample[ rdd:sparkparallelize[sc; !10] ;0b;0.7;1]

/ TODO: fix below
sparkrddname rdd:sparkparallelize[sc; !10]
/ #<JavaRDD: qiss.core/spark-parallelize, core.cljc, L.2381>
sparkrddname[rdd;"heythisismyrddname"]
/ No matching method found: setName for class java.lang.String

sparkcollect sparkdistinct rdd:sparkparallelize[sc; (1; 1; 2; 3; 4; 5)]


// the following two lines are equivalent
sparkcollect rdd:sparkparallelizepairs[sc;10#sparktuple[1;2]]
sparkcollect sc sparkparallelizepairs 10#1 sparktuple 2

/ qiss operators with rdd (and PairRDD) as first-class-type
*rdd             / sparkfirst
rdd,rdd          / sparkjoin
3#rdd            / sparktake
show rdd         / sparkcollect
?rdd             / sparkdistinct
#(?rdd,rdd)      / sparkcount

/ create 2.6 million paird
prdd: sc sparkparallelizepairs ({(x sparktuple 1 ? 10)}'10000#alpha)
show (=prdd)     / sparkgroupbykeys
show (!prdd)     / sparkkeys
show (.prdd)     / sparkvalues

sparkcollect rdd sparkcartesian rdd

filteredrdd:sparkfilter[numdata;{0=x mod 2}]
sparkcollect filteredrdd

sparkstop[sc]



ilsfile:sparktextfile[sc;"iowa-liquor.csv"]  / http://mishadoff.com/blog/spark-in-clojure/
sparkfirst ilsfile

data:sparkparallelize[sc; ("a"; "b"; "c"; "d"; "e")]    / parallized collection

sparkfirst data       / take a peek at the first item in the rdd


sparkcollect sparkmap[numdata; {x * x}]   / melt down

data:sparkparallelizepairs[sc; (("a";1) ;("b";2); ("c";3); ("d";4); ("e";5))]

sparkfirst data       / the same


rddtext:sparktextfile[sc;"poi_names.txt"]

rddtext2:sparkmap[rddtext;{#x}]  / serialization of this count function causes a spark meltdown

/ be patient as spark will eventually return

/ isn't rddtext'#
/ rddtext speach # better?
/ maybe qiss should auto-magically spark-erize RDD transformations and actions?

sparkreduce[rddtext2;+]
